{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Loading of OpenAI API Key from `.env` file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c2383fd038ab4db"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:15:59.978264Z",
     "start_time": "2023-12-17T13:15:59.941454Z"
    }
   },
   "id": "9d10d379a836f7a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A simple parameterized LLM call"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a788f9eafe49ca5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI  # Chat API\n",
    "# from langchain.llms import OpenAI  # Legacy text-only API\n",
    "\n",
    "# Prompt Abstraction\n",
    "from langchain.prompts import BaseChatPromptTemplate  # not needed, this is simply the base-type for Chat Prompt Templates\n",
    "from langchain.prompts import ChatPromptTemplate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:00.234686Z",
     "start_time": "2023-12-17T13:15:59.980109Z"
    }
   },
   "id": "3d2bc8ca6c01a236"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", # default\n",
    "                   # model=\"gpt-4\", # GPT4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:00.277496Z",
     "start_time": "2023-12-17T13:16:00.256701Z"
    }
   },
   "id": "806a1016f281e498"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call model directly\n",
    "model.invoke(\"Tell me joke about cats\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:01.800891Z",
     "start_time": "2023-12-17T13:16:00.281084Z"
    }
   },
   "id": "dc031949cac5ccac"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "prompt: BaseChatPromptTemplate = ChatPromptTemplate.from_template(\"\"\"Tell me a joke about {topic}\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:01.811560Z",
     "start_time": "2023-12-17T13:16:01.806412Z"
    }
   },
   "id": "8cc06d1f6af777d5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[HumanMessage(content='Tell me a joke about tigers')])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate instantiation of prompt templates by passing required parameters\n",
    "prompt.invoke({\"topic\": \"tigers\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:01.814101Z",
     "start_time": "2023-12-17T13:16:01.810915Z"
    }
   },
   "id": "b7ec336ab9aca245"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important insight:\n",
    "\n",
    "All the essential building blocks provided by LangChain (models, prompts, retrievers) have a common interface.\n",
    " \n",
    "They all support, amongst others, the following methods:\n",
    "\n",
    "```python\n",
    "# invoke\n",
    "# stream\n",
    "# batch\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afed8555bb8c2c6d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='Why was the tiger late to the party?\\n\\nBecause he got caught up in a \"spotted\" traffic jam!')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt.invoke({\"topic\": \"tigers\"}))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:03.435415Z",
     "start_time": "2023-12-17T13:16:01.815385Z"
    }
   },
   "id": "23f652537a3ac72e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "We can express the same sequence more elegantly using LCEL syntax. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bb6b7160bc23418"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:03.439712Z",
     "start_time": "2023-12-17T13:16:03.436175Z"
    }
   },
   "id": "4411290aebe6eccb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't goldfish like to play basketball?\\n\\nBecause they're afraid of getting caught in the net!\")"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\"topic\": \"goldfish\"})\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:04.584766Z",
     "start_time": "2023-12-17T13:16:03.441725Z"
    }
   },
   "id": "5acdb0fef68a8613"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we wish to obtain only the text, we can do `result.content`.  \n",
    "But there is a more elegant way to include this transformation directly as part of the chain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7887f66244a0f59"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:04.596614Z",
     "start_time": "2023-12-17T13:16:04.586001Z"
    }
   },
   "id": "d0b02b2321fb61b5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='This is a test.')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage\n",
    "\n",
    "ai_message = AIMessage(content=\"This is a test.\")\n",
    "ai_message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:04.619831Z",
     "start_time": "2023-12-17T13:16:04.600583Z"
    }
   },
   "id": "388c228ed53bff75"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'This is a test.'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsers support the same interface, we `invoke` them\n",
    "parser.invoke(ai_message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:04.621381Z",
     "start_time": "2023-12-17T13:16:04.604809Z"
    }
   },
   "id": "35fc117acc6fdf60"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:04.622180Z",
     "start_time": "2023-12-17T13:16:04.606983Z"
    }
   },
   "id": "e5686e954533f7e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the lemon go to the doctor?\n",
      "\n",
      "Because it wasn't peeling well!\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"topic\": \"lemons\"}))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:06.714594Z",
     "start_time": "2023-12-17T13:16:04.610602Z"
    }
   },
   "id": "2f644d6719ba73c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Passing in strings rather than mappings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fc779e93aab37d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far we always called the chain by invoking it with a dictionary.  \n",
    "What, if we simply wish to pass in a text?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44f5ad6819e8dbf6"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected mapping type as input to ChatPromptTemplate. Received <class 'str'>.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  chain.invoke(\"lemons\")\n",
    "except TypeError as err:\n",
    "  print(err)\n",
    "# We receive a `TypeError`. This is expected."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:06.735991Z",
     "start_time": "2023-12-17T13:16:06.716895Z"
    }
   },
   "id": "38aae1e14f11b7d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We receive an error as the chain expects a mapping type.  \n",
    "We can verify this by looking at the expected input schema:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9b150c2c5577a11"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"title\": \"PromptInput\", \"type\": \"object\", \"properties\": {\"topic\": {\"title\": \"Topic\", \"type\": \"string\"}}}'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema_json()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:06.737716Z",
     "start_time": "2023-12-17T13:16:06.725650Z"
    }
   },
   "id": "ea1307b47a3595f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The schema is displayed in `JSON Schema` format. \n",
    "\n",
    "We can read this as \"The input schema for the first segment of the chain, titled _PromptInput_, expects to receive an `object` (i.e. a Python dict)^ with one property (i.e. attribute) `topic` which in turn is of type `string`. So the input should be of the form `{\"topic\": my_topic: str}`.\"\n",
    "\n",
    "^Note:  \n",
    "In JSON Schema, specifying `\"type\": \"object\"` means that the value is expected to be a JSON object. An object in JSON is a collection of key/value pairs, where each key is a string, and the value can be of any type, including another object. A JSON object is thus analogous to a dictionary in Python. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bf2b9f3f43b7fd0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, in order to call out chain with a string, we need a way to transform the input string into a mapping type that the prompt knows how to consume.\n",
    "\n",
    "We can do this as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2080186894dc3eeb"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:06.739020Z",
     "start_time": "2023-12-17T13:16:06.730406Z"
    }
   },
   "id": "f9beaa520d96d1cf"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "chain = RunnableParallel(topic=RunnablePassthrough()) | prompt | model | parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:06.768945Z",
     "start_time": "2023-12-17T13:16:06.733585Z"
    }
   },
   "id": "f6007eadbefafcc4"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'Why did the goldfish bring a suitcase to the party?\\n\\nBecause he wanted to travel in \"fin\" style!'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"goldfish\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:07.904570Z",
     "start_time": "2023-12-17T13:16:06.737109Z"
    }
   },
   "id": "a1b30cca1aba6ba1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Including more than one variable"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd4c9b6885c592c6"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Tell me something about {topic}.\n",
    "\n",
    "Answer in {language}.\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:07.916602Z",
     "start_time": "2023-12-17T13:16:07.906377Z"
    }
   },
   "id": "962755c124b85bfb"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "chain = prompt | model | parser\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:07.917810Z",
     "start_time": "2023-12-17T13:16:07.909926Z"
    }
   },
   "id": "f2c9ebf53f94f9fb"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'Limoenen zijn citrusvruchten die bekend staan om hun zure smaak en verfrissende aroma. Ze zijn rijk aan vitamine C en bevatten ook antioxidanten die goed zijn voor de gezondheid. Limoenen worden vaak gebruikt in verschillende gerechten en drankjes, zoals limonade, cocktails en desserts. Ze zijn ook een populaire smaakmaker in de keuken en worden vaak gebruikt om een frisse en zure toets aan gerechten toe te voegen.'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Familiar way of invoking the chain\n",
    "chain.invoke({\"topic\": \"lemons\", \"language\": \"Dutch\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:11.630744Z",
     "start_time": "2023-12-17T13:16:07.914026Z"
    }
   },
   "id": "9959a071ce8d8a7f"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemons, ook wel citroenen genoemd, zijn gele vruchten die behoren tot de citrusfamilie. Ze zijn zuur van smaak en staan bekend om hun verfrissende aroma. Lemons bevatten veel vitamine C en worden vaak gebruikt in de keuken als smaakmaker in gerechten, drankjes en desserts. Daarnaast hebben ze ook verschillende gezondheidsvoordelen, zoals het versterken van het immuunsysteem en het bevorderen van de spijsvertering."
     ]
    }
   ],
   "source": [
    "# Making use of the `stream` method\n",
    "for token in chain.stream({\"topic\": \"lemons\", \"language\": \"Dutch\"}):\n",
    "  print(token, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:15.559145Z",
     "start_time": "2023-12-17T13:16:11.633022Z"
    }
   },
   "id": "76a66505f3633b59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This works since, with the introduction of the LangChain Expression Language, all the core abstractions LangChain provides (models, prompts, parsers, retrievers, etc.) all implement the `Runnable` protocol and thus guarantee support for `invoke`, `batch`, `stream`, as well as asynchronous counterparts for these. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bda5c83f01ef547"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explicit Input & Output Schema"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e4c0be808c6f686"
  },
  {
   "cell_type": "markdown",
   "source": [
    "What else do Runnables offer that is of value for us?\n",
    "\n",
    "A: They have an explicit schema definition of what they expect as input and output."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef909e22a47ae0e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"title\": \"PromptInput\", \"type\": \"object\", \"properties\": {\"language\": {\"title\": \"Language\", \"type\": \"string\"}, \"topic\": {\"title\": \"Topic\", \"type\": \"string\"}}}'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema_json()\n",
    "# Our chain expects a dictionary with `language` and `topic` attributes as input"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:15.566877Z",
     "start_time": "2023-12-17T13:16:15.561276Z"
    }
   },
   "id": "665ac6b9f0db90e1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"title\": \"StrOutputParserOutput\", \"type\": \"string\"}'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.output_schema.schema_json()\n",
    "# And will return a result of type `string`"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:15.567813Z",
     "start_time": "2023-12-17T13:16:15.564737Z"
    }
   },
   "id": "4df985df9e3b8042"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"title\": \"ChatOpenAIOutput\", \"anyOf\": [{\"$ref\": \"#/definitions/AIMessage\"}, {\"$ref\": \"#/definitions/HumanMessage\"}, {\"$ref\": \"#/definitions/ChatMessage\"}, {\"$ref\": \"#/definitions/SystemMessage\"}, {\"$ref\": \"#/definitions/FunctionMessage\"}, {\"$ref\": \"#/definitions/ToolMessage\"}], \"definitions\": {\"AIMessage\": {\"title\": \"AIMessage\", \"description\": \"A Message from an AI.\", \"type\": \"object\", \"properties\": {\"content\": {\"title\": \"Content\", \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"object\"}]}}]}, \"additional_kwargs\": {\"title\": \"Additional Kwargs\", \"type\": \"object\"}, \"type\": {\"title\": \"Type\", \"default\": \"ai\", \"enum\": [\"ai\"], \"type\": \"string\"}, \"example\": {\"title\": \"Example\", \"default\": false, \"type\": \"boolean\"}}, \"required\": [\"content\"]}, \"HumanMessage\": {\"title\": \"HumanMessage\", \"description\": \"A Message from a human.\", \"type\": \"object\", \"properties\": {\"content\": {\"title\": \"Content\", \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"object\"}]}}]}, \"additional_kwargs\": {\"title\": \"Additional Kwargs\", \"type\": \"object\"}, \"type\": {\"title\": \"Type\", \"default\": \"human\", \"enum\": [\"human\"], \"type\": \"string\"}, \"example\": {\"title\": \"Example\", \"default\": false, \"type\": \"boolean\"}}, \"required\": [\"content\"]}, \"ChatMessage\": {\"title\": \"ChatMessage\", \"description\": \"A Message that can be assigned an arbitrary speaker (i.e. role).\", \"type\": \"object\", \"properties\": {\"content\": {\"title\": \"Content\", \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"object\"}]}}]}, \"additional_kwargs\": {\"title\": \"Additional Kwargs\", \"type\": \"object\"}, \"type\": {\"title\": \"Type\", \"default\": \"chat\", \"enum\": [\"chat\"], \"type\": \"string\"}, \"role\": {\"title\": \"Role\", \"type\": \"string\"}}, \"required\": [\"content\", \"role\"]}, \"SystemMessage\": {\"title\": \"SystemMessage\", \"description\": \"A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages.\", \"type\": \"object\", \"properties\": {\"content\": {\"title\": \"Content\", \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"object\"}]}}]}, \"additional_kwargs\": {\"title\": \"Additional Kwargs\", \"type\": \"object\"}, \"type\": {\"title\": \"Type\", \"default\": \"system\", \"enum\": [\"system\"], \"type\": \"string\"}}, \"required\": [\"content\"]}, \"FunctionMessage\": {\"title\": \"FunctionMessage\", \"description\": \"A Message for passing the result of executing a function back to a model.\", \"type\": \"object\", \"properties\": {\"content\": {\"title\": \"Content\", \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"object\"}]}}]}, \"additional_kwargs\": {\"title\": \"Additional Kwargs\", \"type\": \"object\"}, \"type\": {\"title\": \"Type\", \"default\": \"function\", \"enum\": [\"function\"], \"type\": \"string\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"content\", \"name\"]}, \"ToolMessage\": {\"title\": \"ToolMessage\", \"description\": \"A Message for passing the result of executing a tool back to a model.\", \"type\": \"object\", \"properties\": {\"content\": {\"title\": \"Content\", \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"array\", \"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"object\"}]}}]}, \"additional_kwargs\": {\"title\": \"Additional Kwargs\", \"type\": \"object\"}, \"type\": {\"title\": \"Type\", \"default\": \"tool\", \"enum\": [\"tool\"], \"type\": \"string\"}, \"tool_call_id\": {\"title\": \"Tool Call Id\", \"type\": \"string\"}}, \"required\": [\"content\", \"tool_call_id\"]}}}'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also expect the schema of an interim part of our chain\n",
    "(prompt | model).output_schema.schema_json()\n",
    "# The model is guaranteed to return a Result of type AIMessage, HumanMessage, SystemMessage, ChatMessage, FunctionMessage or ToolMessage.\n",
    "# Usually it will be AIMessage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:15.572123Z",
     "start_time": "2023-12-17T13:16:15.570342Z"
    }
   },
   "id": "2e89fd9abf0a44a6"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a6bf568230d58be5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A simple RAG Implementation\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation and currently is the de-facto architectural pattern to use for grounding an LLM system in custom data. The idea is that we provide the model with the relevant context to answer the question and guide it to use (only) that context for the answer generation.\n",
    "\n",
    "In order to obtain the relevant fragments of context, a special form of representation (embeddings) is used, that enables a semantic proximity search to obtain the fragments from the corpus that are most relevant for answering the question at hand."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1a3acdd73264de6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To implement RAG we can re-use many LangChain building blocks we are familiar with already.\n",
    "> We have already seen the usage of prompts, models and output parsers.\n",
    " \n",
    "In addition, we now introduce the following new building blocks:\n",
    "\n",
    "Embeddings – these allow us to transform text to a corresponding semantically searchable representation\n",
    "Chroma – a Vector Database for storing embeddings, as well as easy semantic search & retrieval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d73c4c3bafee46a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need a way to \"preserve\" certain attributes as they flow through the chain.  \n",
    "For example we now need to utilize the **question** string twice. Once as input to our retriever for identifying the semantically most relevant pieces of context information. And another time, to pass the question to the model, together with the context, for answer generation.\n",
    "\n",
    "LangChain allows us to do this using either `RunnablePassthrough`, or `operator.itemgetter`. \n",
    "I find the usage of `operator.itemgetter` more explicit and easier to reason about and use it in the example below: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e402799f1538999f"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# Embed a few sentences as a test\n",
    "vectorstore = Chroma.from_texts(\n",
    "    [\"Arno loves Python\", \n",
    "     \"Fran loves Wool\",\n",
    "     \"Tais enjoys separation of concerns.\",\n",
    "     \"Squash makes Corine happy.\",\n",
    "     \"Hans likes Philosophy.\"],\n",
    "    collection_name=\"demo-data\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "# Note: For details on search kwargs see\n",
    "# https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.as_retriever\n",
    "\n",
    "# RAG prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# RAG chain\n",
    "chain = (\n",
    "        {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        } \n",
    "        | prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:45.815386Z",
     "start_time": "2023-12-17T13:16:45.392275Z"
    }
   },
   "id": "eb34cf231de8e19f"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Squash makes Corine happy.'),\n Document(page_content='Squash makes Corine happy.')]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's verify that the retriever can obtain the relevant context fragments\n",
    "retriever.invoke(\"What does Corine like?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:48.601517Z",
     "start_time": "2023-12-17T13:16:48.388276Z"
    }
   },
   "id": "4f0850a70da780d0"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Optionally, to observe what is happening at each step of the chain, we can set langchain debug mode to `True` \n",
    "from langchain.globals import set_debug\n",
    "# set_debug(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:49.811511Z",
     "start_time": "2023-12-17T13:16:49.806846Z"
    }
   },
   "id": "c5d14d9476dc0b91"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'Corine likes squash.'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What does Corine like?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:52.079202Z",
     "start_time": "2023-12-17T13:16:50.555696Z"
    }
   },
   "id": "8a981eab26b238c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we wish to simply pass in a question, without the dictionary we can use the trick from before, using `RunnableParallel` and `RunnablePassthrough` to convert a string into the required shape as part of the chain. \n",
    "\n",
    "We also see that the chain objects themselves are composable since they, too, are of type `Runnable`. So if we have more complicated logic, we can break it down into smaller sub-chains that are easier to test and fine-tune in isolation and then use them to construct the final chain:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7988de8b70809162"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "chain_v2 = (\n",
    "    RunnableParallel(question=RunnablePassthrough()) # This converts string to dict\n",
    "    | \n",
    "    chain\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:54.056468Z",
     "start_time": "2023-12-17T13:16:54.052424Z"
    }
   },
   "id": "7ea71bb0072adf6f"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "'Arno likes Python.'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_v2.invoke(\"What does Arno like?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T13:16:55.968510Z",
     "start_time": "2023-12-17T13:16:54.812503Z"
    }
   },
   "id": "67bbdf8631ff252b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This concludes the material covered in the Dec '23 PAIWeb #1 Meetup.  \n",
    "The material will be made available on GitHub and also shared in our Zulip community.\n",
    "\n",
    "If you have questions or would like to explore the topics covered more deeply, \n",
    "feel warmly welcome to ask and follow-up questions and engage in conversation on Zulip. :-)\n",
    "\n",
    "I am looking forward to what we can learn and build together! "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bb1cef1982c0865"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
